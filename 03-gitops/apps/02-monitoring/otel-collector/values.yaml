nameOverride: otel-collector
fullnameOverride: otel-collector
mode: deployment

image:
  repository: ghcr.io/open-telemetry/opentelemetry-collector-releases/opentelemetry-collector-contrib
  tag: 0.120.0

config:
  connectors:
    spanmetrics: { }
  extensions:
    health_check:
      endpoint: ${env:MY_POD_IP}:13133
  processors:
    batch: { }
    k8sattributes:
      extract:
        metadata:
          - k8s.namespace.name
          - k8s.deployment.name
          - k8s.statefulset.name
          - k8s.daemonset.name
          - k8s.cronjob.name
          - k8s.job.name
          - k8s.node.name
          - k8s.pod.name
          - k8s.pod.uid
          - k8s.pod.start_time
      passthrough: false
      pod_association:
        - sources:
            - from: resource_attribute
              name: k8s.pod.ip
        - sources:
            - from: resource_attribute
              name: k8s.pod.uid
        - sources:
            - from: connection
    memory_limiter:
      check_interval: 5s
      limit_percentage: 80
      spike_limit_percentage: 25
    resource:
      attributes:
        - action: insert
          from_attribute: k8s.pod.uid
          key: service.instance.id
    transform:
      error_mode: ignore
      trace_statements:
        - context: span
          statements:
            - replace_pattern(name, "\\?.*", "")
            - replace_match(name, "GET /api/products/*", "GET /api/products/{productId}")

##############################################

  receivers:
    httpcheck/frontend-proxy:
      targets:
        - endpoint: http://frontend-proxy:8080
    otlp:
      protocols:
        grpc: {}
        http: {}
    redis:
      collection_interval: 10s
      endpoint: valkey-cart.otel-demo.svc.cluster.local:6379

##############################################

  exporters:
    debug: {}
    otlp/jaeger:
      endpoint: jaeger-collector.monitoring:4317
      tls:
        insecure: true
    otlp/tempo:
      endpoint: tempo.monitoring.svc.cluster.local:4317
      tls:
        insecure: true
    loki:
      endpoint: http://loki-stack.monitoring.svc.cluster.local:3100/loki/api/v1/push
    prometheus:
      endpoint: "0.0.0.0:8889"

##############################################

  service:
    extensions:
    - health_check
    pipelines:
      logs:
        receivers: [ otlp ]
        processors: [ k8sattributes, memory_limiter, resource, batch ]
        exporters: [ loki ]
      metrics:
        receivers: [ httpcheck/frontend-proxy, redis, otlp, spanmetrics ]
        processors: [ k8sattributes, memory_limiter, resource, batch ]
        exporters: [ prometheus ]
      traces:
        receivers: [otlp]
        processors: [k8sattributes, memory_limiter, resource, transform, batch]
        exporters: [spanmetrics, otlp/jaeger, otlp/tempo]
    telemetry:
      metrics:
        address: "0.0.0.0:8888"


##############################################

ports:
  metrics:
    enabled: true
    containerPort: 8888
    servicePort: 8888
  spanmetrics:
    enabled: true
    containerPort: 8889
    servicePort: 8889

service:
  enabled: true
  type: ClusterIP


serviceMonitor:
  enabled: true
  metricsEndpoints:
    - port: spanmetrics
      interval: 15s
    - port: metrics
      interval: 15s
  extraLabels:
    release: kube-prometheus-stack